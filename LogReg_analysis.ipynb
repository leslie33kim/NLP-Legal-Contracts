{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyIcDUykwCNe"
      },
      "source": [
        "L2-regularized logistic regression for binary or multiclass classification; trains a model (on `train.txt`), optimizes L2 regularization strength on `dev.txt`, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals and prints out the strongest coefficients for each class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My Drive/INFO 159"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgi_eBXBwCxz",
        "outputId": "ac1f79aa-ee0e-4ea9-c67a-aece599baeb8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/INFO 159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import operator\n",
        "import nltk\n",
        "import math\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4KuVSCSqlUX",
        "outputId": "51a80b1c-b56b-4fc0-9f58-56241a9a567d",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QA1kOslxwCNk"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[2].lstrip().rstrip()\n",
        "            text = cols[3]\n",
        "\n",
        "            X.append(text)\n",
        "            Y.append(label)\n",
        "\n",
        "    return X, Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, trainX, trainY, devX, devY, testX, testY):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.min_feature_count=2\n",
        "        self.log_reg = None\n",
        "\n",
        "        self.trainY=trainY\n",
        "        self.devY=devY\n",
        "        self.testY=testY\n",
        "        \n",
        "        self.trainX = self.process(trainX, training=True)\n",
        "        self.devX = self.process(devX, training=False)\n",
        "        self.testX = self.process(testX, training=False)\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append(feats)\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, X_data, training = False):\n",
        "        \n",
        "        data = self.featurize(X_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        for idx, feats in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def train(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        best_dev_accuracy=0\n",
        "        best_model=None\n",
        "        for C in [0.1, 1, 10, 100]:\n",
        "            self.log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n",
        "            self.log_reg.fit(self.trainX, self.trainY)\n",
        "            training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "            development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "            if development_accuracy > best_dev_accuracy:\n",
        "                best_dev_accuracy=development_accuracy\n",
        "                best_model=self.log_reg\n",
        "\n",
        "#             print(\"C: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (C, training_accuracy, development_accuracy))\n",
        "\n",
        "        self.log_reg=best_model\n",
        "        \n",
        "\n",
        "    def test(self):\n",
        "        return self.log_reg.score(self.testX, self.testY)\n",
        "        \n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atzpSwmfwK-C",
        "outputId": "4c3f088c-1e37-42af-ae62-bb5fa04a8a58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "2rAAnnuOwtEw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def action_verbs(words, bigrams, trigrams, feats, stemmer):\n",
        "  action_verbs = ['must', 'shall', 'will', 'shall', 'may', \n",
        "                  'can', 'right', 'option', 'terminate', 'reasonable', \n",
        "                  'good', 'faith', 'audit', 'examine', 'first', \n",
        "                  'prior', 'own', 'expense', 'negotiate', 'valuable',\n",
        "                  'commercially', 'irrevocable', 'perpetual', 'neither',\n",
        "                  'liable', 'indirect', 'damage', 'waive', 'suit',\n",
        "                  'action', 'claim', 'loss', 'demand', 'liability',\n",
        "                  'cost', 'expense', 'written', 'notice', 'immediately'\n",
        "                  'written', 'notice', 'agreement', 'time', \n",
        "                  'any', 'amended', 'without', 'penalty', 'fee',\n",
        "                  'not', 'obligation', 'request', 'notice', 'inspect',\n",
        "                  'inspection', 'auditing', 'responsible', 'partially',\n",
        "                  'partial', 'only', 'exclusive', 'exception', 'pay',\n",
        "                  'exceed', 'maximum', 'event', 'no', 'total', \n",
        "                  'worldwide', 'sublicensable', 'royalty', 'free', \n",
        "                  'non-exclusive', 'non', 'license', 'independent', \n",
        "                  'directly', 'indirectly', 'not', 'contain', 'limitation',\n",
        "                  'permitted', 'prohibited', 'consent', 'circumstance', 'restriction',\n",
        "                  'profit', 'entitled', 'recover', 'limited', 'seek', 'breach',\n",
        "                  'omission', 'penalty', 'access', 'non-transferable', 'cancellation',\n",
        "                  'transferable', 'limited', 'consent', 'notify', 'compliance', ''\n",
        "                  ]\n",
        "  synonyms = []\n",
        "  for verb in action_verbs:\n",
        "    for syn in wn.synsets(verb):\n",
        "        for l in syn.lemmas():\n",
        "          synonyms.append(l.name())\n",
        "  action_verbs = list(set(action_verbs + synonyms))\n",
        "  action_verbs = set([stemmer.stem(verb.lower()) for verb in action_verbs])\n",
        "\n",
        "  for word in words:\n",
        "    word = stemmer.stem(word.lower())\n",
        "    if word in action_verbs:\n",
        "      feats[word] = 1\n",
        "\n",
        "\n",
        "  bigrams_list = [\n",
        "      ('only', 'responsible'), ('partially', 'permitted'),\n",
        "      ('conditionally', 'responsible'), ('exclusive', 'right'),\n",
        "      ('with', 'exception'), ('aggregate', 'liability'),\n",
        "      ('exclusive', 'access'), ('non', 'transferable'),\n",
        "      ('non', 'sublicensable'), ('non', 'exclusive'),\n",
        "      ('no', 'restrictions'), ('prior', 'notice'),\n",
        "      ('written', 'notice'), ('prior', 'consent'),\n",
        "      ('damages', 'incurred'), ('liability', 'cap'),\n",
        "      ('not', 'permitted'), ('independent', 'certified'),\n",
        "      ('cancellation', 'fee'), ('good', 'faith'),\n",
        "      ('own', 'expense'), ('may', 'terminate'),\n",
        "      ('no', 'liability'), ('own', 'expense'),\n",
        "      ('no', 'obligation')\n",
        "  ]\n",
        "  bigrams_list = [(stemmer.stem(pairs[0]), stemmer.stem(pairs[1])) for pairs in bigrams_list]\n",
        "  \n",
        "  for bi in bigrams:\n",
        "    bi = (stemmer.stem(bi[0].lower()), stemmer.stem(bi[1].lower()))\n",
        "    if bi in bigrams_list:\n",
        "      feats[bi[0] + \"_\" + bi[1]] = 1\n",
        "\n",
        "\n",
        "  trigrams_list = [\n",
        "      ('unless', 'otherwise', 'agreed'), ('under', 'no', 'circumstances'),\n",
        "      ('in', 'no', 'event'), ('at', 'no', 'time'),\n",
        "      ('independent', '3rd', 'party'), ('independent', 'third', 'party'),\n",
        "      ('any', 'and', 'all'), ('optional', 'prior', 'right')\n",
        "  ]\n",
        "  trigrams_list = [(stemmer.stem(triplet[0]), stemmer.stem(triplet[1]), stemmer.stem(triplet[2])) for triplet in trigrams_list]\n",
        "\n",
        "  for tri in trigrams:\n",
        "    tri = (stemmer.stem(tri[0].lower()), stemmer.stem(tri[1].lower()),  stemmer.stem(tri[2].lower()))\n",
        "    if tri in trigrams_list:\n",
        "      feats[tri[0] + \"_\" + tri[1] + \"_\" + tri[2]] = 1\n",
        "  return feats"
      ],
      "metadata": {
        "id": "7UQfJyilwVp1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def penalty_fee(words, bigrams, trigrams, feats, stemmer):\n",
        "  penalty = [\"penalty\", \"fee\", \"repercussion\", \"consequence\", \"breach\", \"prohibited\", \"disallow\", \"concurrent\",\n",
        "           \"entitle\", \"breach\", \"non-compliance\"]\n",
        "  audit = [\"audit\", \"prior notice\", \"written notice\", r\"\\d+ day notice\"]\n",
        "  synonyms = []\n",
        "  for p in penalty:\n",
        "    for syn in wn.synsets(p):\n",
        "        for l in syn.lemmas():\n",
        "          synonyms.append(l.name())\n",
        "  updated_pen = list(set(penalty + synonyms + audit))\n",
        "  updated_pen = set([stemmer.stem(p.lower()) for p in penalty])\n",
        "\n",
        "  for word in words:\n",
        "    word = stemmer.stem(word.lower())\n",
        "    if word in updated_pen:\n",
        "      feats[word] = 1\n",
        "  return feats"
      ],
      "metadata": {
        "id": "kMBtthK_whbo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FDrtqioAwCNn"
      },
      "outputs": [],
      "source": [
        "def binary_bow_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "    bigrams = list(nltk.bigrams(text.lower().split()))\n",
        "    trigrams = list(nltk.trigrams(text.lower().split()))\n",
        "    #lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    feats = action_verbs(words, bigrams, trigrams, feats, stemmer)\n",
        "    feats = penalty_fee(words, bigrams, trigrams, feats, stemmer)\n",
        "            \n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WLne-riAwCNn"
      },
      "outputs": [],
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    critical_value=(1-significance_level)/2\n",
        "    z_alpha=-1*norm.ppf(critical_value)\n",
        "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
        "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZlRrNtp6wCNo"
      },
      "outputs": [],
      "source": [
        "def run(trainingFile, devFile, testFile):\n",
        "    trainX, trainY=load_data(trainingFile)\n",
        "    devX, devY=load_data(devFile)\n",
        "    testX, testY=load_data(testFile)\n",
        "    \n",
        "    simple_classifier = Classifier(binary_bow_featurize, trainX, trainY, devX, devY, testX, testY)\n",
        "    simple_classifier.train()\n",
        "    accuracy=simple_classifier.test()\n",
        "    \n",
        "    lower, upper=confidence_intervals(accuracy, len(testY), .95)\n",
        "    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n",
        "\n",
        "    simple_classifier.printWeights()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU7K5tZ5wCNp",
        "outputId": "5302b544-37e3-4aef-ca94-90ce5b9f31bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for best dev model: 0.624, 95% CIs: [0.529 0.718]\n",
            "\n",
            "DP\t0.507\tnon-exclus\n",
            "DP\t0.434\texceed\n",
            "DP\t0.378\tpermit\n",
            "DP\t0.337\tsole\n",
            "DP\t0.308\tnon-transfer\n",
            "DP\t0.238\texcept\n",
            "DP\t0.228\tliabil\n",
            "DP\t0.224\tworldwid\n",
            "DP\t0.221\troyalti\n",
            "DP\t0.220\tlicens\n",
            "\n",
            "MP\t0.588\texclus\n",
            "MP\t0.284\tmust\n",
            "MP\t0.280\tterm\n",
            "MP\t0.220\tnot\n",
            "MP\t0.205\tani\n",
            "MP\t0.204\ttake\n",
            "MP\t0.196\texclus_right\n",
            "MP\t0.189\tpay\n",
            "MP\t0.183\texpir\n",
            "MP\t0.173\tcase\n",
            "\n",
            "NP\t0.324\tnegoti\n",
            "NP\t0.289\tliabl\n",
            "NP\t0.284\tfirst\n",
            "NP\t0.267\tmay\n",
            "NP\t0.262\tspecial\n",
            "NP\t0.231\thave\n",
            "NP\t0.212\tcommerci\n",
            "NP\t0.191\tset\n",
            "NP\t0.190\tadvis\n",
            "NP\t0.161\tamend\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainingFile = \"splits/train.txt\"\n",
        "devFile = \"splits/dev.txt\"\n",
        "testFile = \"splits/test.txt\"\n",
        "    \n",
        "run(trainingFile, devFile, testFile)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QBm5QqzjwCNp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LmQgM9JYwCNp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}